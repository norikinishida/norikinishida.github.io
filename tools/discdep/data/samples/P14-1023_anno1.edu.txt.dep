{
    "root": [
        {
            "id": 0,
            "parent": -1,
            "text": "ROOT",
            "relation": "null"
        },
        {
            "id": 1,
            "parent": 6,
            "text": "Context-predicting models \r",
            "relation": "BACKGROUND"
        },
        {
            "id": 2,
            "parent": 1,
            "text": "( more commonly known as embeddings or neural language models ) \r",
            "relation": "DEFINITION"
        },
        {
            "id": 3,
            "parent": 1,
            "text": "are the new kids on the distributional semantics block . <S>\r",
            "relation": "SAME-UNIT"
        },
        {
            "id": 4,
            "parent": 6,
            "text": "Despite the buzz \r",
            "relation": "CONTRAST"
        },
        {
            "id": 5,
            "parent": 4,
            "text": "surrounding these models , \r",
            "relation": "ELABORATION"
        },
        {
            "id": 6,
            "parent": 7,
            "text": "the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches . <S>\r",
            "relation": "BACKGROUND"
        },
        {
            "id": 7,
            "parent": 0,
            "text": "In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings . <S>\r",
            "relation": "ROOT"
        },
        {
            "id": 8,
            "parent": 9,
            "text": "The results , to our own surprise , show \r",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 9,
            "parent": 7,
            "text": "that the buzz is fully justified , \r",
            "relation": "ELABORATION"
        },
        {
            "id": 10,
            "parent": 9,
            "text": "as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts . <S>\r",
            "relation": "CAUSE-RESULT-REASON"
        }
    ]
}