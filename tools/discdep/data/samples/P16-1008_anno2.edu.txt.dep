{
    "root": [
        {
            "id": 0,
            "parent": -1,
            "text": "ROOT",
            "relation": "null"
        },
        {
            "id": 1,
            "parent": 6,
            "text": "Attention mechanism has enhanced stateof-the-art Neural Machine Translation ( NMT ) ",
            "relation": "BACKGROUND"
        },
        {
            "id": 2,
            "parent": 1,
            "text": "by jointly learning to align and translate . <S>",
            "relation": "MANNER-MEANS"
        },
        {
            "id": 3,
            "parent": 1,
            "text": "It tends to ignore past alignment information , ",
            "relation": "ELABORATION"
        },
        {
            "id": 4,
            "parent": 3,
            "text": "however , which often leads to over-translation and under-translation . <S>",
            "relation": "CONTRAST"
        },
        {
            "id": 5,
            "parent": 6,
            "text": "To address this problem , ",
            "relation": "ENABLEMENT"
        },
        {
            "id": 6,
            "parent": 0,
            "text": "we propose coverage-based NMT in this paper . <S>",
            "relation": "ROOT"
        },
        {
            "id": 7,
            "parent": 6,
            "text": "We maintain a coverage vector ",
            "relation": "MANNER-MEANS"
        },
        {
            "id": 8,
            "parent": 7,
            "text": "to keep track of the attention history . <S>",
            "relation": "ENABLEMENT"
        },
        {
            "id": 9,
            "parent": 7,
            "text": "The coverage vector is fed to the attention model ",
            "relation": "ELABORATION"
        },
        {
            "id": 10,
            "parent": 9,
            "text": "to help adjust future attention , ",
            "relation": "ENABLEMENT"
        },
        {
            "id": 11,
            "parent": 10,
            "text": "which lets NMT system to consider more about untranslated source words . <S>",
            "relation": "ELABORATION"
        },
        {
            "id": 12,
            "parent": 13,
            "text": "Experiments show ",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 13,
            "parent": 6,
            "text": "that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT . <S>",
            "relation": "EVALUATION"
        }
    ]
}