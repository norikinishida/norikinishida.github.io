{
    "root": [
        {
            "id": 0,
            "parent": -1,
            "text": "ROOT",
            "relation": "null"
        },
        {
            "id": 1,
            "parent": 0,
            "text": "We introduce a novel approach \r",
            "relation": "ROOT"
        },
        {
            "id": 2,
            "parent": 1,
            "text": "for building language models \r",
            "relation": "ELABORATION"
        },
        {
            "id": 3,
            "parent": 1,
            "text": "based on a systematic , recursive exploration of skip n-gram models \r",
            "relation": "BACKGROUND"
        },
        {
            "id": 4,
            "parent": 3,
            "text": "which are interpolated \r",
            "relation": "ELABORATION"
        },
        {
            "id": 5,
            "parent": 4,
            "text": "using modified Kneser-Ney smoothing . <S>\r",
            "relation": "MANNER-MEANS"
        },
        {
            "id": 6,
            "parent": 1,
            "text": "Our approach generalizes language models \r",
            "relation": "ELABORATION"
        },
        {
            "id": 7,
            "parent": 6,
            "text": "as it contains the classical interpolation with lower order models as a special case . <S>\r",
            "relation": "CAUSE-RESULT-REASON"
        },
        {
            "id": 8,
            "parent": 1,
            "text": "In this paper we motivate , formalize and present our approach . <S>\r",
            "relation": "ELABORATION"
        },
        {
            "id": 9,
            "parent": 10,
            "text": "In an extensive empirical experiment over English text corpora we demonstrate \r",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 10,
            "parent": 1,
            "text": "that our generalized language models lead to a substantial reduction of perplexity between 3.1 % and 12.7 % \r",
            "relation": "EVALUATION"
        },
        {
            "id": 11,
            "parent": 10,
            "text": "in comparison to traditional language models \r",
            "relation": "COMPARISON"
        },
        {
            "id": 12,
            "parent": 11,
            "text": "using modified Kneser-Ney smoothing . <S>\r",
            "relation": "ELABORATION"
        },
        {
            "id": 13,
            "parent": 1,
            "text": "Furthermore , we investigate the behaviour over three other languages and a domain specific corpus \r",
            "relation": "EVALUATION"
        },
        {
            "id": 14,
            "parent": 13,
            "text": "where we observed consistent improvements . <S>\r",
            "relation": "ELABORATION"
        },
        {
            "id": 15,
            "parent": 16,
            "text": "Finally , we also show \r",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 16,
            "parent": 1,
            "text": "that the strength of our approach lies in its ability \r",
            "relation": "EVALUATION"
        },
        {
            "id": 17,
            "parent": 16,
            "text": "to cope in particular with sparse training data . <S>\r",
            "relation": "ELABORATION"
        },
        {
            "id": 18,
            "parent": 19,
            "text": "Using a very small training data set of only 736 KB text \r",
            "relation": "MANNER-MEANS"
        },
        {
            "id": 19,
            "parent": 16,
            "text": "we yield improvements of even 25.7 % reduction of perplexity . <S>\r",
            "relation": "CAUSE-RESULT-REASON"
        }
    ]
}