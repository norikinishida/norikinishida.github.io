{
    "root": [
        {
            "id": 0,
            "parent": -1,
            "text": "ROOT",
            "relation": "null"
        },
        {
            "id": 1,
            "parent": 0,
            "text": "We use single-agent and multi-agent Reinforcement Learning ( RL ) \r",
            "relation": "ROOT"
        },
        {
            "id": 2,
            "parent": 1,
            "text": "for learning dialogue policies in a resource allocation negotiation scenario . <S>\r",
            "relation": "ENABLEMENT"
        },
        {
            "id": 3,
            "parent": 1,
            "text": "Two agents learn concurrently \r",
            "relation": "ELABORATION"
        },
        {
            "id": 4,
            "parent": 3,
            "text": "by interacting with each other \r",
            "relation": "MANNER-MEANS"
        },
        {
            "id": 5,
            "parent": 4,
            "text": "without any need for simulated users ( SUs ) \r",
            "relation": "CONDITION"
        },
        {
            "id": 6,
            "parent": 5,
            "text": "to train against \r",
            "relation": "ENABLEMENT"
        },
        {
            "id": 7,
            "parent": 5,
            "text": "or corpora \r",
            "relation": "JOINT"
        },
        {
            "id": 8,
            "parent": 7,
            "text": "to learn from . <S>\r",
            "relation": "ELABORATION"
        },
        {
            "id": 9,
            "parent": 1,
            "text": "In particular , we compare the Q-learning , Policy Hill-Climbing ( PHC ) and Win or Learn Fast Policy Hill-Climbing ( PHC-WoLF ) algorithms , \r",
            "relation": "ELABORATION"
        },
        {
            "id": 10,
            "parent": 9,
            "text": "varying the scenario complexity ( state space size ) , the number of training episodes , the learning rate , and the exploration rate . <S>\r",
            "relation": "CONDITION"
        },
        {
            "id": 11,
            "parent": 12,
            "text": "Our results show \r",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 12,
            "parent": 9,
            "text": "that generally Q-learning fails to converge \r",
            "relation": "CAUSE-RESULT-REASON"
        },
        {
            "id": 13,
            "parent": 12,
            "text": "whereas PHC and PHC-WoLF always converge and perform similarly . <S>\r",
            "relation": "CONTRAST"
        },
        {
            "id": 14,
            "parent": 15,
            "text": "We also show \r",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 15,
            "parent": 12,
            "text": "that very high gradually decreasing exploration rates are required for convergence . <S>\r",
            "relation": "JOINT"
        },
        {
            "id": 16,
            "parent": 1,
            "text": "We conclude \r",
            "relation": "ELABORATION"
        },
        {
            "id": 17,
            "parent": 16,
            "text": "that multi-agent RL of dialogue policies is a promising alternative \r",
            "relation": "ATTRIBUTION"
        },
        {
            "id": 18,
            "parent": 17,
            "text": "to using single-agent RL and SUs \r",
            "relation": "ENABLEMENT"
        },
        {
            "id": 19,
            "parent": 18,
            "text": "or learning directly from corpora . <S>\r",
            "relation": "JOINT"
        }
    ]
}